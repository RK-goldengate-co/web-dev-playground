#!/usr/bin/env python3
"""
Python Web Scraper v√† Data Extractor
C√¥ng c·ª• scraping v√† tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ c√°c website
"""

import requests
import json
import csv
import time
import random
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Any, Optional, Tuple
import argparse
import logging
from dataclasses import dataclass, asdict
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import pandas as pd

# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('web_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ScrapingConfig:
    """C·∫•u h√¨nh scraping"""
    base_url: str
    user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    delay_between_requests: float = 1.0
    max_retries: int = 3
    timeout: int = 30
    respect_robots_txt: bool = True

    def __post_init__(self):
        if not self.base_url.startswith(('http://', 'https://')):
            self.base_url = 'https://' + self.base_url

@dataclass
class ScrapedData:
    """D·ªØ li·ªáu ƒë√£ scrape"""
    url: str
    title: str
    content: str
    metadata: Dict[str, Any]
    scraped_at: str
    status_code: int

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

class WebScraper:
    """L·ªõp ch√≠nh cho web scraping"""

    def __init__(self, config: ScrapingConfig):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })

        # Kh·ªüi t·∫°o Selenium driver n·∫øu c·∫ßn
        self.driver = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver:
            self.driver.quit()

    def setup_selenium(self, headless: bool = True):
        """Thi·∫øt l·∫≠p Selenium WebDriver"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument("--headless")

        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument(f"user-agent={self.config.user_agent}")

        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            logger.info("‚úÖ Kh·ªüi t·∫°o Selenium WebDriver th√†nh c√¥ng")
        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi t·∫°o Selenium: {e}")
            self.driver = None

    def get_page(self, url: str, retries: int = None) -> Optional[str]:
        """L·∫•y n·ªôi dung trang web"""
        if retries is None:
            retries = self.config.max_retries

        for attempt in range(retries):
            try:
                logger.info(f"ƒêang l·∫•y trang: {url} (l·∫ßn th·ª≠ {attempt + 1})")

                response = self.session.get(
                    url,
                    timeout=self.config.timeout,
                    allow_redirects=True
                )

                response.raise_for_status()

                # Delay ƒë·ªÉ tr√°nh b·ªã rate limit
                time.sleep(self.config.delay_between_requests)

                return response.text

            except requests.exceptions.RequestException as e:
                logger.warning(f"L·ªói l·∫•y trang (l·∫ßn th·ª≠ {attempt + 1}): {e}")

                if attempt < retries - 1:
                    wait_time = (attempt + 1) * 2  # Exponential backoff
                    logger.info(f"ƒê·ª£i {wait_time} gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"Kh√¥ng th·ªÉ l·∫•y trang sau {retries} l·∫ßn th·ª≠: {url}")

        return None

    def parse_html(self, html: str, url: str) -> ScrapedData:
        """Ph√¢n t√≠ch HTML v√† tr√≠ch xu·∫•t d·ªØ li·ªáu"""
        soup = BeautifulSoup(html, 'html.parser')

        # Lo·∫°i b·ªè script v√† style
        for script in soup(["script", "style"]):
            script.decompose()

        # L·∫•y ti√™u ƒë·ªÅ
        title = soup.title.string if soup.title else "No title"

        # L·∫•y n·ªôi dung ch√≠nh
        content_selectors = [
            'article', '.content', '.post-content', '.entry-content',
            'main', '.main-content', '#content', '.article-body'
        ]

        content = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(strip=True)
                break

        if not content:
            # L·∫•y to√†n b·ªô text n·∫øu kh√¥ng t√¨m th·∫•y content ch√≠nh
            content = soup.get_text(strip=True)

        # Tr√≠ch xu·∫•t metadata
        metadata = {
            'url': url,
            'title': title,
            'description': self.extract_meta_description(soup),
            'keywords': self.extract_meta_keywords(soup),
            'author': self.extract_author(soup),
            'publish_date': self.extract_publish_date(soup),
            'images': self.extract_images(soup, url),
            'links': self.extract_links(soup, url),
            'headings': self.extract_headings(soup),
        }

        return ScrapedData(
            url=url,
            title=title,
            content=content,
            metadata=metadata,
            scraped_at=datetime.now().isoformat(),
            status_code=200
        )

    def extract_meta_description(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t meta description"""
        meta_desc = soup.find('meta', attrs={'name': 'description'}) or \
                   soup.find('meta', attrs={'property': 'og:description'})
        return meta_desc.get('content', '') if meta_desc else ''

    def extract_meta_keywords(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t meta keywords"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        return meta_keywords.get('content', '') if meta_keywords else ''

    def extract_author(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t t√™n t√°c gi·∫£"""
        author_selectors = [
            'meta[name="author"]', '.author', '.byline', '.post-author',
            '[rel="author"]', '.entry-author'
        ]

        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                if author_elem.name == 'meta':
                    return author_elem.get('content', '')
                else:
                    return author_elem.get_text(strip=True)

        return ''

    def extract_publish_date(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t ng√†y xu·∫•t b·∫£n"""
        date_selectors = [
            'meta[property="article:published_time"]',
            'meta[name="publishdate"]',
            '.publish-date', '.post-date', '.entry-date',
            'time[datetime]', 'time'
        ]

        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                if date_elem.name == 'meta':
                    return date_elem.get('content', '')
                elif date_elem.get('datetime'):
                    return date_elem.get('datetime', '')
                else:
                    return date_elem.get_text(strip=True)

        return ''

    def extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Tr√≠ch xu·∫•t danh s√°ch h√¨nh ·∫£nh"""
        images = []
        img_tags = soup.find_all('img', src=True)

        for img in img_tags:
            src = img.get('src', '')
            if src:
                # Chuy·ªÉn relative URL th√†nh absolute URL
                absolute_url = urljoin(base_url, src)
                images.append(absolute_url)

        return images

    def extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Tr√≠ch xu·∫•t danh s√°ch li√™n k·∫øt"""
        links = []
        link_tags = soup.find_all('a', href=True)

        for link in link_tags:
            href = link.get('href', '')
            if href:
                absolute_url = urljoin(base_url, href)
                links.append(absolute_url)

        return list(set(links))  # Lo·∫°i b·ªè duplicate

    def extract_headings(self, soup: BeautifulSoup) -> Dict[str, List[str]]:
        """Tr√≠ch xu·∫•t c√°c ti√™u ƒë·ªÅ"""
        headings = {'h1': [], 'h2': [], 'h3': [], 'h4': [], 'h5': [], 'h6': []}

        for level in headings.keys():
            heading_tags = soup.find_all(level)
            headings[level] = [tag.get_text(strip=True) for tag in heading_tags]

        return headings

    def scrape_single_page(self, url: str) -> Optional[ScrapedData]:
        """Scrape m·ªôt trang ƒë∆°n"""
        html_content = self.get_page(url)
        if html_content:
            return self.parse_html(html_content, url)
        return None

    def scrape_multiple_pages(self, urls: List[str]) -> List[ScrapedData]:
        """Scrape nhi·ªÅu trang"""
        results = []

        for url in urls:
            logger.info(f"ƒêang scrape: {url}")
            data = self.scrape_single_page(url)
            if data:
                results.append(data)
            else:
                logger.warning(f"Kh√¥ng th·ªÉ scrape: {url}")

        return results

    def crawl_website(self, start_url: str, max_pages: int = 50,
                     max_depth: int = 3) -> List[ScrapedData]:
        """Crawl to√†n b·ªô website"""
        visited = set()
        to_visit = [(start_url, 0)]  # (url, depth)
        results = []

        while to_visit and len(results) < max_pages:
            current_url, depth = to_visit.pop(0)

            if current_url in visited or depth > max_depth:
                continue

            visited.add(current_url)

            logger.info(f"Crawling: {current_url} (depth: {depth})")

            data = self.scrape_single_page(current_url)
            if data:
                results.append(data)

                # T√¨m li√™n k·∫øt n·ªôi b·ªô ƒë·ªÉ ti·∫øp t·ª•c crawl
                if depth < max_depth:
                    html_content = self.get_page(current_url)
                    if html_content:
                        soup = BeautifulSoup(html_content, 'html.parser')
                        internal_links = self.get_internal_links(soup, start_url)

                        for link in internal_links:
                            if link not in visited:
                                to_visit.append((link, depth + 1))

            # Delay gi·ªØa c√°c request
            time.sleep(self.config.delay_between_requests)

        return results

    def get_internal_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """L·∫•y danh s√°ch li√™n k·∫øt n·ªôi b·ªô"""
        internal_links = []
        link_tags = soup.find_all('a', href=True)

        parsed_base = urlparse(base_url)

        for link in link_tags:
            href = link.get('href', '')
            absolute_url = urljoin(base_url, href)

            # Ch·ªâ l·∫•y li√™n k·∫øt n·ªôi b·ªô (c√πng domain)
            parsed_link = urlparse(absolute_url)
            if parsed_link.netloc == parsed_base.netloc:
                internal_links.append(absolute_url)

        return list(set(internal_links))

    def save_to_json(self, data_list: List[ScrapedData], filename: str):
        """L∆∞u d·ªØ li·ªáu d·∫°ng JSON"""
        json_data = [data.to_dict() for data in data_list]

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ƒê√£ l∆∞u {len(data_list)} b·∫£n ghi v√†o {filename}")

    def save_to_csv(self, data_list: List[ScrapedData], filename: str):
        """L∆∞u d·ªØ li·ªáu d·∫°ng CSV"""
        fieldnames = ['url', 'title', 'content', 'scraped_at', 'status_code']

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

            for data in data_list:
                writer.writerow({
                    'url': data.url,
                    'title': data.title,
                    'content': data.content[:500] + '...' if len(data.content) > 500 else data.content,
                    'scraped_at': data.scraped_at,
                    'status_code': data.status_code
                })

        logger.info(f"ƒê√£ l∆∞u {len(data_list)} b·∫£n ghi v√†o {filename}")

    def export_to_excel(self, data_list: List[ScrapedData], filename: str):
        """Xu·∫•t d·ªØ li·ªáu d·∫°ng Excel"""
        df_data = []

        for data in data_list:
            row = {
                'URL': data.url,
                'Title': data.title,
                'Content': data.content[:1000] + '...' if len(data.content) > 1000 else data.content,
                'Description': data.metadata.get('description', ''),
                'Author': data.metadata.get('author', ''),
                'Publish Date': data.metadata.get('publish_date', ''),
                'Scraped At': data.scraped_at,
                'Status Code': data.status_code
            }
            df_data.append(row)

        df = pd.DataFrame(df_data)

        try:
            df.to_excel(filename, index=False, engine='openpyxl')
            logger.info(f"ƒê√£ xu·∫•t {len(data_list)} b·∫£n ghi v√†o {filename}")
        except ImportError:
            logger.error("openpyxl kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t. Kh√¥ng th·ªÉ xu·∫•t Excel.")

class NewsScraper(WebScraper):
    """Scraper chuy√™n d·ª•ng cho tin t·ª©c"""

    def scrape_news_article(self, url: str) -> Optional[ScrapedData]:
        """Scrape b√†i b√°o tin t·ª©c"""
        return self.scrape_single_page(url)

    def scrape_news_category(self, category_url: str, max_articles: int = 10) -> List[ScrapedData]:
        """Scrape danh m·ª•c tin t·ª©c"""
        html = self.get_page(category_url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        articles = []

        # T√¨m c√°c li√™n k·∫øt b√†i vi·∫øt
        article_selectors = [
            '.news-item a', '.article-item a', '.post-item a',
            'h2 a', 'h3 a', '.title a'
        ]

        for selector in article_selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if href and href.startswith(('http', '/')):
                    article_url = urljoin(category_url, href)

                    # Ki·ªÉm tra xem ƒë√£ ƒë·ªß b√†i ch∆∞a
                    if len(articles) >= max_articles:
                        break

                    # Scrape b√†i vi·∫øt
                    article_data = self.scrape_news_article(article_url)
                    if article_data:
                        articles.append(article_data)

            if len(articles) >= max_articles:
                break

        return articles[:max_articles]

class EcommerceScraper(WebScraper):
    """Scraper chuy√™n d·ª•ng cho e-commerce"""

    def scrape_product(self, url: str) -> Optional[Dict[str, Any]]:
        """Scrape th√¥ng tin s·∫£n ph·∫©m"""
        html = self.get_page(url)
        if not html:
            return None

        soup = BeautifulSoup(html, 'html.parser')

        product_info = {
            'url': url,
            'name': self.extract_product_name(soup),
            'price': self.extract_product_price(soup),
            'description': self.extract_product_description(soup),
            'images': self.extract_product_images(soup, url),
            'category': self.extract_product_category(soup),
            'brand': self.extract_product_brand(soup),
            'availability': self.extract_product_availability(soup),
            'specifications': self.extract_product_specs(soup),
            'scraped_at': datetime.now().isoformat()
        }

        return product_info

    def extract_product_name(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t t√™n s·∫£n ph·∫©m"""
        selectors = ['h1', '.product-title', '.product-name', '.title']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        return ''

    def extract_product_price(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t gi√° s·∫£n ph·∫©m"""
        selectors = ['.price', '.product-price', '.current-price', '.amount']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        return ''

    def extract_product_description(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t m√¥ t·∫£ s·∫£n ph·∫©m"""
        selectors = ['.description', '.product-description', '.content']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        return ''

    def extract_product_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """Tr√≠ch xu·∫•t h√¨nh ·∫£nh s·∫£n ph·∫©m"""
        images = []
        img_selectors = ['.product-image img', '.gallery img', '.image img']

        for selector in img_selectors:
            img_tags = soup.select(selector)
            for img in img_tags:
                src = img.get('src') or img.get('data-src')
                if src:
                    absolute_url = urljoin(base_url, src)
                    images.append(absolute_url)

        return images

    def extract_product_category(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t danh m·ª•c s·∫£n ph·∫©m"""
        selectors = ['.category', '.breadcrumb a', '.category-link']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        return ''

    def extract_product_brand(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t th∆∞∆°ng hi·ªáu"""
        selectors = ['.brand', '.manufacturer', '.vendor']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        return ''

    def extract_product_availability(self, soup: BeautifulSoup) -> str:
        """Tr√≠ch xu·∫•t t√¨nh tr·∫°ng c√≤n h√†ng"""
        selectors = ['.availability', '.stock', '.in-stock', '.out-of-stock']
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        return 'Unknown'

    def extract_product_specs(self, soup: BeautifulSoup) -> Dict[str, str]:
        """Tr√≠ch xu·∫•t th√¥ng s·ªë k·ªπ thu·∫≠t"""
        specs = {}
        spec_selectors = ['.specifications', '.specs', '.attributes']

        for selector in spec_selectors:
            spec_table = soup.select_one(selector)
            if spec_table:
                rows = spec_table.find_all(['tr', 'li', 'div'])
                for row in rows:
                    if ':' in row.get_text():
                        key, value = row.get_text().split(':', 1)
                        specs[key.strip()] = value.strip()

        return specs

def main():
    """H√†m ch√≠nh"""
    parser = argparse.ArgumentParser(description='Web Scraper Tool')
    parser.add_argument('url', help='URL c·∫ßn scrape')
    parser.add_argument('--mode', choices=['single', 'crawl', 'news', 'product'],
                       default='single', help='Ch·∫ø ƒë·ªô scraping')
    parser.add_argument('--output', '-o', default='scraped_data.json',
                       help='File ƒë·∫ßu ra')
    parser.add_argument('--format', choices=['json', 'csv', 'excel'],
                       default='json', help='ƒê·ªãnh d·∫°ng ƒë·∫ßu ra')
    parser.add_argument('--max-pages', type=int, default=10,
                       help='S·ªë trang t·ªëi ƒëa khi crawl')
    parser.add_argument('--delay', type=float, default=1.0,
                       help='Delay gi·ªØa c√°c request (gi√¢y)')
    parser.add_argument('--headless', action='store_true',
                       help='Ch·∫°y Selenium headless mode')

    args = parser.parse_args()

    # C·∫•u h√¨nh scraper
    config = ScrapingConfig(
        base_url=args.url,
        delay_between_requests=args.delay
    )

    with WebScraper(config) as scraper:
        # Thi·∫øt l·∫≠p Selenium n·∫øu c·∫ßn
        if args.mode in ['news', 'product']:
            scraper.setup_selenium(headless=args.headless)

        results = []

        if args.mode == 'single':
            # Scrape trang ƒë∆°n
            data = scraper.scrape_single_page(args.url)
            if data:
                results = [data]

        elif args.mode == 'crawl':
            # Crawl website
            results = scraper.crawl_website(args.url, max_pages=args.max_pages)

        elif args.mode == 'news':
            # Scrape tin t·ª©c
            news_scraper = NewsScraper(config)
            results = news_scraper.scrape_news_category(args.url)

        elif args.mode == 'product':
            # Scrape s·∫£n ph·∫©m
            ecommerce_scraper = EcommerceScraper(config)
            product_data = ecommerce_scraper.scrape_product(args.url)
            if product_data:
                results = [ScrapedData(
                    url=product_data['url'],
                    title=product_data['name'],
                    content=product_data['description'],
                    metadata=product_data,
                    scraped_at=datetime.now().isoformat(),
                    status_code=200
                )]

        if results:
            # L∆∞u k·∫øt qu·∫£
            if args.format == 'json':
                scraper.save_to_json(results, args.output)
            elif args.format == 'csv':
                scraper.save_to_csv(results, args.output.replace('.json', '.csv'))
            elif args.format == 'excel':
                scraper.export_to_excel(results, args.output.replace('.json', '.xlsx'))

            print(f"‚úÖ ƒê√£ scrape th√†nh c√¥ng {len(results)} m·ª•c")
            print(f"üìÅ K·∫øt qu·∫£ ƒë√£ l∆∞u v√†o: {args.output}")

            # Hi·ªÉn th·ªã th·ªëng k√™
            print("üìä Th·ªëng k√™:")
            print(f"   - T·ªïng s·ªë k√Ω t·ª± n·ªôi dung: {sum(len(r.content) for r in results)}")
            print(f"   - S·ªë h√¨nh ·∫£nh t√¨m th·∫•y: {sum(len(r.metadata.get('images', [])) for r in results)}")
        else:
            print("‚ùå Kh√¥ng scrape ƒë∆∞·ª£c d·ªØ li·ªáu n√†o")

if __name__ == "__main__":
    main()
