#!/usr/bin/env python3
"""
Python Data Science v√† Machine Learning
·ª®ng d·ª•ng ph√¢n t√≠ch d·ªØ li·ªáu v·ªõi pandas, numpy, scikit-learn, v√† matplotlib
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# Set style cho matplotlib
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class DataAnalyzer:
    """L·ªõp ph√¢n t√≠ch d·ªØ li·ªáu to√†n di·ªán"""

    def __init__(self, data_path=None):
        self.data = None
        self.processed_data = None
        self.models = {}
        self.scaler = StandardScaler()

        if data_path:
            self.load_data(data_path)

    def load_data(self, path, separator=','):
        """T·∫£i d·ªØ li·ªáu t·ª´ file CSV"""
        try:
            self.data = pd.read_csv(path, sep=separator)
            print(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu t·ª´ {path}")
            print(f"üìä K√≠ch th∆∞·ªõc: {self.data.shape}")
            print(f"üìã C√°c c·ªôt: {list(self.data.columns)}")
            return True
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i d·ªØ li·ªáu: {e}")
            return False

    def explore_data(self):
        """Kh√°m ph√° d·ªØ li·ªáu c∆° b·∫£n"""
        if self.data is None:
            print("‚ùå Ch∆∞a t·∫£i d·ªØ li·ªáu")
            return

        print("\nüìä TH√îNG TIN T·ªîNG QUAN:")
        print("=" * 50)
        print(self.data.info())

        print("\nüìà TH·ªêNG K√ä M√î T·∫¢:")
        print("=" * 50)
        print(self.data.describe())

        print("\nüîç KI·ªÇM TRA GI√Å TR·ªä NULL:")
        print("=" * 50)
        null_counts = self.data.isnull().sum()
        print(null_counts[null_counts > 0])

        print("\nüìä PH√ÇN PH·ªêI D·ªÆ LI·ªÜU:")
        print("=" * 50)
        for col in self.data.columns:
            if self.data[col].dtype in ['int64', 'float64']:
                print(f"{col}: {self.data[col].skew():.2f} (skewness)")

    def clean_data(self):
        """L√†m s·∫°ch d·ªØ li·ªáu"""
        if self.data is None:
            print("‚ùå Ch∆∞a t·∫£i d·ªØ li·ªáu")
            return

        self.processed_data = self.data.copy()

        # X·ª≠ l√Ω gi√° tr·ªã null
        for col in self.processed_data.columns:
            if self.processed_data[col].isnull().sum() > 0:
                if self.processed_data[col].dtype in ['int64', 'float64']:
                    # ƒêi·ªÅn gi√° tr·ªã trung b√¨nh cho s·ªë
                    mean_val = self.processed_data[col].mean()
                    self.processed_data[col].fillna(mean_val, inplace=True)
                    print(f"‚úÖ ƒêi·ªÅn gi√° tr·ªã trung b√¨nh {mean_val:.2f} cho c·ªôt {col}")
                else:
                    # ƒêi·ªÅn gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t cho category
                    mode_val = self.processed_data[col].mode()[0]
                    self.processed_data[col].fillna(mode_val, inplace=True)
                    print(f"‚úÖ ƒêi·ªÅn gi√° tr·ªã ph·ªï bi·∫øn '{mode_val}' cho c·ªôt {col}")

        # Lo·∫°i b·ªè d·ªØ li·ªáu tr√πng l·∫∑p
        duplicates = self.processed_data.duplicated().sum()
        if duplicates > 0:
            self.processed_data.drop_duplicates(inplace=True)
            print(f"‚úÖ Lo·∫°i b·ªè {duplicates} b·∫£n ghi tr√πng l·∫∑p")

        print("‚úÖ L√†m s·∫°ch d·ªØ li·ªáu ho√†n th√†nh!")

    def visualize_data(self):
        """Tr·ª±c quan h√≥a d·ªØ li·ªáu"""
        if self.processed_data is None:
            print("‚ùå Ch∆∞a x·ª≠ l√Ω d·ªØ li·ªáu")
            return

        # Thi·∫øt l·∫≠p figure
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Ph√¢n t√≠ch d·ªØ li·ªáu tr·ª±c quan', fontsize=16, fontweight='bold')

        # Histogram cho t·∫•t c·∫£ bi·∫øn s·ªë
        numeric_cols = self.processed_data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            self.processed_data[numeric_cols].hist(ax=axes[0, 0], bins=20, alpha=0.7)
            axes[0, 0].set_title('Ph√¢n b·ªë c√°c bi·∫øn s·ªë')
            axes[0, 0].tick_params(axis='x', rotation=45)

        # Box plot ƒë·ªÉ ph√°t hi·ªán outliers
        if len(numeric_cols) > 0:
            sns.boxplot(data=self.processed_data[numeric_cols], ax=axes[0, 1])
            axes[0, 1].set_title('Box plot - Ph√°t hi·ªán outliers')
            axes[0, 1].tick_params(axis='x', rotation=45)

        # Correlation heatmap
        if len(numeric_cols) > 1:
            corr_matrix = self.processed_data[numeric_cols].corr()
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                       square=True, ax=axes[1, 0])
            axes[1, 0].set_title('Ma tr·∫≠n t∆∞∆°ng quan')

        # Scatter plot cho m·ªëi quan h·ªá gi·ªØa 2 bi·∫øn ƒë·∫ßu ti√™n
        if len(numeric_cols) >= 2:
            x_col, y_col = numeric_cols[0], numeric_cols[1]
            axes[1, 1].scatter(self.processed_data[x_col], self.processed_data[y_col], alpha=0.6)
            axes[1, 1].set_xlabel(x_col)
            axes[1, 1].set_ylabel(y_col)
            axes[1, 1].set_title(f'M·ªëi quan h·ªá {x_col} vs {y_col}')

        plt.tight_layout()
        plt.savefig('data_analysis_report.png', dpi=300, bbox_inches='tight')
        plt.show()

    def perform_clustering(self, n_clusters=3):
        """Th·ª±c hi·ªán ph√¢n c·ª•m K-Means"""
        if self.processed_data is None:
            print("‚ùå Ch∆∞a x·ª≠ l√Ω d·ªØ li·ªáu")
            return

        # Chu·∫©n b·ªã d·ªØ li·ªáu cho clustering
        numeric_data = self.processed_data.select_dtypes(include=[np.number])

        if numeric_data.empty:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu s·ªë ƒë·ªÉ ph√¢n c·ª•m")
            return

        # Chu·∫©n h√≥a d·ªØ li·ªáu
        scaled_data = self.scaler.fit_transform(numeric_data)

        # Th·ª±c hi·ªán K-Means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(scaled_data)

        # Th√™m k·∫øt qu·∫£ v√†o d·ªØ li·ªáu
        self.processed_data['Cluster'] = clusters

        print(f"‚úÖ Ph√¢n c·ª•m ho√†n th√†nh v·ªõi {n_clusters} c·ª•m")
        print(f"üìä K√≠ch th∆∞·ªõc c√°c c·ª•m: {np.bincount(clusters)}")

        # Tr·ª±c quan h√≥a k·∫øt qu·∫£
        plt.figure(figsize=(10, 6))
        scatter = plt.scatter(scaled_data[:, 0], scaled_data[:, 1],
                            c=clusters, cmap='viridis', alpha=0.6)
        plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                   c='red', marker='X', s=200, label='Centers')
        plt.title('K·∫øt qu·∫£ ph√¢n c·ª•m K-Means')
        plt.xlabel('Feature 1 (scaled)')
        plt.ylabel('Feature 2 (scaled)')
        plt.legend()
        plt.colorbar(scatter)
        plt.savefig('clustering_results.png', dpi=300, bbox_inches='tight')
        plt.show()

        return clusters

    def train_classification_model(self, target_column, test_size=0.2):
        """Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n lo·∫°i"""
        if self.processed_data is None:
            print("‚ùå Ch∆∞a x·ª≠ l√Ω d·ªØ li·ªáu")
            return

        if target_column not in self.processed_data.columns:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt m·ª•c ti√™u: {target_column}")
            return

        # Chu·∫©n b·ªã d·ªØ li·ªáu
        X = self.processed_data.drop(columns=[target_column])
        y = self.processed_data[target_column]

        # Encode categorical variables
        le = LabelEncoder()
        for col in X.columns:
            if X[col].dtype == 'object':
                X[col] = le.fit_transform(X[col])

        # Chia d·ªØ li·ªáu
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        # Chu·∫©n h√≥a d·ªØ li·ªáu
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # Hu·∫•n luy·ªán m√¥ h√¨nh
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_scaled, y_train)

        # D·ª± ƒëo√°n v√† ƒë√°nh gi√°
        y_pred = model.predict(X_test_scaled)
        accuracy = accuracy_score(y_test, y_pred)

        print("üìä K·∫æT QU·∫¢ M√î H√åNH PH√ÇN LO·∫†I:")
        print("=" * 50)
        print(f"üéØ ƒê·ªô ch√≠nh x√°c: {accuracy:.4f}")
        print(f"üìà Classification Report:\n{classification_report(y_test, y_pred)}")

        # Confusion Matrix
        plt.figure(figsize=(8, 6))
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=np.unique(y), yticklabels=np.unique(y))
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Feature Importance
        if hasattr(model, 'feature_importances_'):
            plt.figure(figsize=(10, 6))
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=True)

            plt.barh(feature_importance['feature'], feature_importance['importance'])
            plt.title('Feature Importance')
            plt.xlabel('Importance')
            plt.tight_layout()
            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
            plt.show()

        self.models['classification'] = model
        return model, accuracy

    def train_regression_model(self, target_column, test_size=0.2):
        """Hu·∫•n luy·ªán m√¥ h√¨nh h·ªìi quy"""
        if self.processed_data is None:
            print("‚ùå Ch∆∞a x·ª≠ l√Ω d·ªØ li·ªáu")
            return

        if target_column not in self.processed_data.columns:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt m·ª•c ti√™u: {target_column}")
            return

        # Chu·∫©n b·ªã d·ªØ li·ªáu
        X = self.processed_data.drop(columns=[target_column])
        y = self.processed_data[target_column]

        # Encode categorical variables
        le = LabelEncoder()
        for col in X.columns:
            if X[col].dtype == 'object':
                X[col] = le.fit_transform(X[col])

        # Chia d·ªØ li·ªáu
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )

        # Chu·∫©n h√≥a d·ªØ li·ªáu
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # Hu·∫•n luy·ªán m√¥ h√¨nh
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train_scaled, y_train)

        # D·ª± ƒëo√°n v√† ƒë√°nh gi√°
        y_pred = model.predict(X_test_scaled)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)

        print("üìä K·∫æT QU·∫¢ M√î H√åNH H·ªíI QUY:")
        print("=" * 50)
        print(f"üìâ Mean Squared Error: {mse:.4f}")
        print(f"üìà Root Mean Squared Error: {rmse:.4f}")
        print(f"üéØ R¬≤ Score: {model.score(X_test_scaled, y_test):.4f}")

        # Bi·ªÉu ƒë·ªì d·ª± ƒëo√°n vs th·ª±c t·∫ø
        plt.figure(figsize=(8, 6))
        plt.scatter(y_test, y_pred, alpha=0.6)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        plt.xlabel('Gi√° tr·ªã th·ª±c t·∫ø')
        plt.ylabel('Gi√° tr·ªã d·ª± ƒëo√°n')
        plt.title('D·ª± ƒëo√°n vs Th·ª±c t·∫ø')
        plt.savefig('regression_results.png', dpi=300, bbox_inches='tight')
        plt.show()

        # Residual plot
        residuals = y_test - y_pred
        plt.figure(figsize=(8, 6))
        plt.scatter(y_pred, residuals, alpha=0.6)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('Gi√° tr·ªã d·ª± ƒëo√°n')
        plt.ylabel('Residuals')
        plt.title('Residual Plot')
        plt.savefig('residual_plot.png', dpi=300, bbox_inches='tight')
        plt.show()

        self.models['regression'] = model
        return model, rmse

    def perform_pca(self, n_components=2):
        """Th·ª±c hi·ªán PCA ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu"""
        if self.processed_data is None:
            print("‚ùå Ch∆∞a x·ª≠ l√Ω d·ªØ li·ªáu")
            return

        # Chu·∫©n b·ªã d·ªØ li·ªáu s·ªë
        numeric_data = self.processed_data.select_dtypes(include=[np.number])

        if numeric_data.shape[1] < 2:
            print("‚ùå C·∫ßn √≠t nh·∫•t 2 c·ªôt s·ªë ƒë·ªÉ th·ª±c hi·ªán PCA")
            return

        # Lo·∫°i b·ªè c·ªôt m·ª•c ti√™u n·∫øu c√≥
        if 'Cluster' in numeric_data.columns:
            numeric_data = numeric_data.drop('Cluster', axis=1)

        # Chu·∫©n h√≥a d·ªØ li·ªáu
        scaled_data = self.scaler.fit_transform(numeric_data)

        # Th·ª±c hi·ªán PCA
        pca = PCA(n_components=n_components)
        pca_result = pca.fit_transform(scaled_data)

        # T·∫°o DataFrame v·ªõi k·∫øt qu·∫£ PCA
        pca_df = pd.DataFrame(
            pca_result,
            columns=[f'PC{i+1}' for i in range(n_components)]
        )

        print("üìä K·∫æT QU·∫¢ PCA:")
        print("=" * 30)
        print(f"üéØ S·ªë chi·ªÅu g·ªëc: {scaled_data.shape[1]}")
        print(f"üéØ S·ªë chi·ªÅu sau PCA: {n_components}")
        print(f"üìà Gi·∫£i th√≠ch ph∆∞∆°ng sai: {pca.explained_variance_ratio_}")
        print(f"üìä T·ªïng ph∆∞∆°ng sai gi·∫£i th√≠ch: {pca.explained_variance_ratio_.sum():.4f}")

        # Tr·ª±c quan h√≥a k·∫øt qu·∫£ PCA
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)

        # Th√™m labels cho c√°c ƒëi·ªÉm n·∫øu c√≥ √≠t d·ªØ li·ªáu
        if len(pca_result) <= 50:
            for i, (x, y) in enumerate(pca_result[:50]):
                plt.annotate(f'Point {i+1}', (x, y), xytext=(5, 5),
                           textcoords='offset points', fontsize=8)

        plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        plt.title('PCA Results - 2D Visualization')
        plt.grid(True, alpha=0.3)
        plt.savefig('pca_results.png', dpi=300, bbox_inches='tight')
        plt.show()

        return pca_df, pca

    def generate_report(self, filename="data_analysis_report.html"):
        """T·∫°o b√°o c√°o ph√¢n t√≠ch d·ªØ li·ªáu"""
        if self.processed_data is None:
            print("‚ùå Ch∆∞a x·ª≠ l√Ω d·ªØ li·ªáu")
            return

        html_content = f"""
        <!DOCTYPE html>
        <html lang="vi">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>B√°o c√°o ph√¢n t√≠ch d·ªØ li·ªáu</title>
            <style>
                body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; line-height: 1.6; }}
                .header {{ text-align: center; color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 20px; }}
                .section {{ margin: 30px 0; }}
                .stats {{ background: #f8f9fa; padding: 20px; border-radius: 8px; margin: 10px 0; }}
                .highlight {{ background: #e8f5e8; padding: 15px; border-left: 4px solid #27ae60; }}
                img {{ max-width: 100%; height: auto; margin: 20px 0; }}
                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
                th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üìä B√°o c√°o ph√¢n t√≠ch d·ªØ li·ªáu</h1>
                <p>T·∫°o b·ªüi Python Data Analysis Tool</p>
                <p><strong>Ng√†y t·∫°o:</strong> {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>

            <div class="section">
                <h2>üìã Th√¥ng tin d·ªØ li·ªáu</h2>
                <div class="stats">
                    <p><strong>K√≠ch th∆∞·ªõc d·ªØ li·ªáu:</strong> {self.processed_data.shape[0]} h√†ng √ó {self.processed_data.shape[1]} c·ªôt</p>
                    <p><strong>Ki·ªÉu d·ªØ li·ªáu:</strong> {dict(self.processed_data.dtypes)}</p>
                </div>
            </div>

            <div class="section">
                <h2>üìà Th·ªëng k√™ m√¥ t·∫£</h2>
                <div class="stats">
                    {self.processed_data.describe().to_html()}
                </div>
            </div>

            <div class="section">
                <h2>üîç Ph√°t hi·ªán v·∫•n ƒë·ªÅ</h2>
                <div class="highlight">
                    <p><strong>Gi√° tr·ªã null:</strong> {self.processed_data.isnull().sum().sum()} gi√° tr·ªã b·ªã thi·∫øu</p>
                    <p><strong>D·ªØ li·ªáu tr√πng l·∫∑p:</strong> {self.processed_data.duplicated().sum()} b·∫£n ghi tr√πng l·∫∑p</p>
                </div>
            </div>

            <div class="section">
                <h2>üìä K·∫øt qu·∫£ ph√¢n t√≠ch</h2>
                <div class="highlight">
                    <p>B√°o c√°o ph√¢n t√≠ch chi ti·∫øt ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o c√°c file h√¨nh ·∫£nh:</p>
                    <ul>
                        <li>data_analysis_report.png - T·ªïng quan d·ªØ li·ªáu</li>
                        <li>clustering_results.png - K·∫øt qu·∫£ ph√¢n c·ª•m</li>
                        <li>confusion_matrix.png - Ma tr·∫≠n nh·∫ßm l·∫´n</li>
                        <li>feature_importance.png - T·∫ßm quan tr·ªçng ƒë·∫∑c tr∆∞ng</li>
                        <li>regression_results.png - K·∫øt qu·∫£ h·ªìi quy</li>
                        <li>pca_results.png - K·∫øt qu·∫£ PCA</li>
                    </ul>
                </div>
            </div>

            <div class="section">
                <h2>üí° Khuy·∫øn ngh·ªã</h2>
                <div class="highlight">
                    <ul>
                        <li>Ki·ªÉm tra v√† x·ª≠ l√Ω outliers n·∫øu c·∫ßn thi·∫øt</li>
                        <li>C√¢n nh·∫Øc lo·∫°i b·ªè c√°c ƒë·∫∑c tr∆∞ng c√≥ t∆∞∆°ng quan cao</li>
                        <li>Thu th·∫≠p th√™m d·ªØ li·ªáu n·∫øu m√¥ h√¨nh ch∆∞a ƒë·ªß ch√≠nh x√°c</li>
                        <li>Th·ª≠ nghi·ªám c√°c m√¥ h√¨nh kh√°c nhau ƒë·ªÉ t√¨m m√¥ h√¨nh t·ªët nh·∫•t</li>
                    </ul>
                </div>
            </div>
        </body>
        </html>
        """

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"‚úÖ B√°o c√°o ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {filename}")

    def export_processed_data(self, filename="processed_data.csv"):
        """Xu·∫•t d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω"""
        if self.processed_data is not None:
            self.processed_data.to_csv(filename, index=False)
            print(f"‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c xu·∫•t v√†o {filename}")
        else:
            print("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ xu·∫•t")


# V√≠ d·ª• s·ª≠ d·ª•ng
if __name__ == "__main__":
    # Kh·ªüi t·∫°o analyzer
    analyzer = DataAnalyzer()

    # V√≠ d·ª• v·ªõi d·ªØ li·ªáu m·∫´u (thay th·∫ø b·∫±ng ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø)
    print("üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH D·ªÆ LI·ªÜU")
    print("=" * 60)

    # T·∫£i d·ªØ li·ªáu m·∫´u (thay th·∫ø b·∫±ng d·ªØ li·ªáu th·ª±c t·∫ø c·ªßa b·∫°n)
    sample_data = pd.DataFrame({
        'age': np.random.normal(30, 10, 100),
        'income': np.random.normal(50000, 15000, 100),
        'score': np.random.normal(70, 15, 100),
        'category': np.random.choice(['A', 'B', 'C'], 100),
        'satisfaction': np.random.choice(['Low', 'Medium', 'High'], 100)
    })

    analyzer.data = sample_data
    analyzer.processed_data = sample_data.copy()

    # Kh√°m ph√° d·ªØ li·ªáu
    analyzer.explore_data()

    # Tr·ª±c quan h√≥a
    print("\nüìä ƒêANG T·∫†O BI·ªÇU ƒê·ªí...")
    analyzer.visualize_data()

    # Ph√¢n c·ª•m
    print("\nüîç ƒêANG TH·ª∞C HI·ªÜN PH√ÇN C·ª§M...")
    analyzer.perform_clustering(n_clusters=3)

    # H·ªìi quy
    print("\nüìà ƒêANG HU·∫§N LUY·ªÜN M√î H√åNH H·ªíI QUY...")
    analyzer.train_regression_model('income')

    # Ph√¢n lo·∫°i
    print("\nüéØ ƒêANG HU·∫§N LUY·ªÜN M√î H√åNH PH√ÇN LO·∫†I...")
    analyzer.train_classification_model('category')

    # PCA
    print("\nüßÆ ƒêANG TH·ª∞C HI·ªÜN PCA...")
    analyzer.perform_pca(n_components=2)

    # T·∫°o b√°o c√°o
    print("\nüìã ƒêANG T·∫†O B√ÅO C√ÅO...")
    analyzer.generate_report()

    print("\n‚úÖ PH√ÇN T√çCH HO√ÄN TH√ÄNH!")
    print("üìÅ Ki·ªÉm tra c√°c file ƒë√£ t·∫°o trong th∆∞ m·ª•c hi·ªán t·∫°i")
